"""Public entry points for running the S1 extraction pipeline."""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Iterable, List, Sequence

from taxonomy.config.settings import Settings
from taxonomy.entities.core import Candidate, Concept
from taxonomy.utils.logging import get_logger, logging_context

from .extractor import ExtractionProcessor
from .io import generate_metadata, load_source_records, write_candidates
from .normalizer import CandidateNormalizer
from .parent_index import ParentIndex
from .processor import S1Processor


def extract_candidates(
    source_records_path: str | Path,
    *,
    level: int,
    previous_parents: Sequence[Candidate | Concept] | None = None,
    output_path: str | Path | None = None,
    metadata_path: str | Path | None = None,
    settings: Settings | None = None,
) -> List[Candidate]:
    """High-level API for running the S1 pipeline.

    Args:
        source_records_path: Path to the JSONL file generated by S0.
        level: Hierarchy level to process (0..3).
        previous_parents: Optional parents emitted by earlier levels to assist
            parent resolution.
        output_path: Optional destination for candidate JSONL output.
        metadata_path: Optional metadata output file.  When omitted the path is
            derived from *output_path* by appending ``.metadata.json``.
        settings: Optional pre-loaded :class:`Settings` instance.
    """

    cfg = settings or Settings()
    label_policy = cfg.policies.label_policy
    extractor = ExtractionProcessor()
    normalizer = CandidateNormalizer(label_policy=label_policy)
    parent_index = ParentIndex(label_policy=label_policy)
    processor = S1Processor(
        extractor=extractor,
        normalizer=normalizer,
        parent_index=parent_index,
    )

    previous_parents = previous_parents or []
    records = list(load_source_records(source_records_path))

    with logging_context(stage="s1", level=level, records=len(records)):
        candidates = processor.process_level(
            records,
            level=level,
            previous_candidates=previous_parents,
        )

    if output_path is not None:
        write_candidates(candidates, output_path)
        if metadata_path is None:
            metadata_path = Path(output_path).with_suffix(".metadata.json")
        stats = {
            "records_in": extractor.metrics.records_in,
            "candidates_out": extractor.metrics.candidates_out,
            "invalid_json": extractor.metrics.invalid_json,
            "quarantined": extractor.metrics.quarantined,
            "provider_errors": extractor.metrics.provider_errors,
            "final_candidates": len(candidates),
        }
        config_used = {
            "policy_version": cfg.policies.policy_version,
            "level": level,
        }
        metadata = generate_metadata(stats, config_used)
        Path(metadata_path).write_text(json.dumps(metadata, indent=2) + "\n", encoding="utf-8")

    return candidates


def _parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run S1 extraction and normalization")
    parser.add_argument("source_records", type=Path, help="Path to S0 JSONL records")
    parser.add_argument("level", type=int, choices=range(0, 4), help="Hierarchy level to process")
    parser.add_argument("output", type=Path, help="Destination JSONL path for candidates")
    parser.add_argument(
        "--parents",
        type=Path,
        help="Optional JSON file containing previously promoted parent candidates",
    )
    parser.add_argument(
        "--metadata",
        type=Path,
        help="Optional metadata output path",
    )
    return parser.parse_args(argv)


def _load_parents(path: Path | None) -> List[Candidate]:
    if path is None or not path.exists():
        return []
    payload = json.loads(path.read_text(encoding="utf-8"))
    return [Candidate.model_validate(item) for item in payload]


def main(argv: Sequence[str] | None = None) -> int:
    args = _parse_args(argv)
    logger = get_logger(module=__name__)

    try:
        parents = _load_parents(args.parents)
        extract_candidates(
            args.source_records,
            level=args.level,
            previous_parents=parents,
            output_path=args.output,
            metadata_path=args.metadata,
        )
    except Exception as exc:  # pragma: no cover - defensive CLI guard
        logger.exception("S1 extraction failed", error=str(exc))
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
