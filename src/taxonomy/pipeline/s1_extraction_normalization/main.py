"""Public entry points for running the S1 extraction pipeline."""

from __future__ import annotations

import argparse
import json
from itertools import islice
from pathlib import Path
from typing import Dict, Iterable, Iterator, List, Sequence, Tuple, TypeVar

from taxonomy.config.settings import Settings
from taxonomy.entities.core import Candidate, Concept
from taxonomy.observability import ObservabilityContext
from taxonomy.utils.logging import get_logger, logging_context

from .extractor import ExtractionProcessor
from .io import generate_metadata, load_source_records
from .normalizer import CandidateNormalizer
from .parent_index import ParentIndex
from .processor import AggregatedCandidate, S1Processor
from taxonomy.utils.helpers import chunked


def extract_candidates(
    source_records_path: str | Path,
    *,
    level: int,
    previous_parents: Sequence[Candidate | Concept] | None = None,
    output_path: str | Path | None = None,
    metadata_path: str | Path | None = None,
    resume_from: str | Path | None = None,
    batch_size: int = 32,
    settings: Settings | None = None,
    observability: ObservabilityContext | None = None,
    audit_mode: bool = False,
) -> List[Candidate]:
    """High-level API for running the S1 pipeline.

    Args:
        source_records_path: Path to the JSONL file generated by S0.
        level: Hierarchy level to process (0..3).
        previous_parents: Optional parents emitted by earlier levels to assist
            parent resolution.
        output_path: Optional destination for candidate JSONL output.
        metadata_path: Optional metadata output file. When omitted the path is
            derived from *output_path* by appending ``.metadata.json``.
        settings: Optional pre-loaded :class:`Settings` instance.
        observability: Optional observability context used for standardized
            metrics, evidence, and quarantine tracking.
    """

    cfg = settings or Settings()
    audit_mode_enabled = bool(audit_mode or cfg.audit_mode.enabled)
    label_policy = cfg.policies.label_policy
    extractor = ExtractionProcessor(observability=observability)
    normalizer = CandidateNormalizer(label_policy=label_policy)
    parent_index = ParentIndex(
        label_policy=label_policy,
        similarity_cutoff=label_policy.parent_similarity_cutoff,
    )
    processor = S1Processor(
        extractor=extractor,
        normalizer=normalizer,
        parent_index=parent_index,
    )

    obs_context = observability or extractor.observability
    snapshot_before = obs_context.snapshot() if obs_context is not None else None
    before_counters = snapshot_before.counters.get("S1", {}) if snapshot_before else {}

    def _latest_sequence(entries: object) -> int:
        """Return the most recent S1 sequence from *entries* without copying."""

        if not isinstance(entries, Sequence) or isinstance(entries, (str, bytes, bytearray)):
            return 0
        for entry in reversed(entries):
            if not isinstance(entry, dict) or entry.get("phase") != "S1":
                continue
            try:
                return int(entry.get("sequence", 0))
            except (TypeError, ValueError):
                continue
        return 0

    def _recent_s1_entries(entries: object, *, baseline: int) -> Iterator[dict[str, object]]:
        """Yield S1 entries with a sequence greater than *baseline*."""

        if not isinstance(entries, Sequence) or isinstance(entries, (str, bytes, bytearray)):
            return
        for entry in reversed(entries):
            if not isinstance(entry, dict) or entry.get("phase") != "S1":
                continue
            try:
                sequence = int(entry.get("sequence", 0))
            except (TypeError, ValueError):
                continue
            if sequence <= baseline:
                break
            yield entry

    before_op_sequence = _latest_sequence(snapshot_before.operations) if snapshot_before else 0
    before_quarantine_sequence = 0
    if snapshot_before:
        before_quarantine_sequence = _latest_sequence(snapshot_before.quarantine.get("items", []))
    previous_parents = list(previous_parents or [])
    if previous_parents:
        parent_index.build_index(previous_parents)

    def _source_iter() -> Iterator:
        records = load_source_records(source_records_path)
        return _limit_source_records(records) if audit_mode_enabled else records

    total_records: int | None = None
    if cfg.observability.precount_s1_records:
        total_records = sum(1 for _ in _source_iter())

    resume_path = Path(resume_from) if resume_from is not None else None
    processed_records, aggregated_state = _load_resume_checkpoint(resume_path)
    if total_records is not None and processed_records > total_records:
        processed_records = total_records

    remaining_records = _skip_records(_source_iter(), processed_records)

    with logging_context(stage="s1", level=level, records=total_records):
        for batch in chunked(remaining_records, batch_size):
            raw = extractor.extract_candidates(batch, level=level, observability=obs_context)
            normalized = normalizer.normalize(raw, level=level)
            aggregated_batch = processor._aggregate(normalized)
            _merge_aggregated_state(aggregated_state, aggregated_batch)
            processed_records += len(batch)
            if resume_path is not None:
                _write_resume_checkpoint(resume_path, processed_records, aggregated_state)

        candidates = processor._materialize(aggregated_state.values())

        output_destination: Path | None = None
        if output_path is not None:
            output_destination = _stream_candidates(candidates, output_path)

        snapshot_after = obs_context.snapshot() if obs_context is not None else None

        def _delta(counter_name: str) -> int:
            if snapshot_after is None:
                return 0
            after_raw = snapshot_after.counters.get("S1", {}).get(counter_name, 0)
            before_raw = before_counters.get(counter_name, 0)
            try:
                after_value = int(after_raw)
            except (TypeError, ValueError):
                after_value = 0
            try:
                before_value = int(before_raw)
            except (TypeError, ValueError):
                before_value = 0
            return after_value - before_value

        provider_errors = 0
        quarantined = 0
        if snapshot_after is not None:
            provider_errors = sum(
                1
                for entry in _recent_s1_entries(snapshot_after.operations, baseline=before_op_sequence)
                if entry.get("operation") == "provider_error"
            )
            quarantined = sum(
                1
                for _ in _recent_s1_entries(
                    snapshot_after.quarantine.get("items", []),
                    baseline=before_quarantine_sequence,
                )
            )

        if snapshot_after is not None:
            stats = {
                "records_in": _delta("records_in"),
                "records_processed_total": processed_records,
                "candidates_out": _delta("candidates_out"),
                "invalid_json": _delta("invalid_json"),
                "quarantined": quarantined,
                "provider_errors": provider_errors,
                "retries": _delta("retries"),
                "final_candidates": len(candidates),
            }
        else:
            metrics = extractor.metrics
            stats = {
                "records_in": metrics.records_in,
                "records_processed_total": processed_records,
                "candidates_out": metrics.candidates_out,
                "invalid_json": metrics.invalid_json,
                "quarantined": metrics.quarantined,
                "provider_errors": metrics.provider_errors,
                "retries": metrics.retries,
                "final_candidates": len(candidates),
            }

        config_used = {
            "policy_version": cfg.policies.policy_version,
            "level": level,
            "batch_size": batch_size,
            "audit_mode": audit_mode_enabled,
        }

        metadata_destination: Path | None
        if metadata_path is not None:
            metadata_destination = Path(metadata_path)
        elif output_destination is not None:
            metadata_destination = Path(f"{output_destination}.metadata.json")
        else:
            metadata_destination = None

        if metadata_destination is not None:
            metadata_destination.parent.mkdir(parents=True, exist_ok=True)
            metadata = generate_metadata(stats, config_used)
            metadata_destination.write_text(json.dumps(metadata, indent=2) + "\n", encoding="utf-8")

    if resume_path is not None:
        _write_resume_checkpoint(resume_path, processed_records, aggregated_state)

    return candidates


T = TypeVar("T")


def _skip_records(records: Iterable, offset: int) -> Iterator:
    for index, record in enumerate(records):
        if index < offset:
            continue
        yield record


def _limit_source_records(records: Iterable[T], *, limit: int = 10) -> Iterator[T]:
    """Yield at most *limit* source records from *records*."""

    return islice(records, limit)


def _merge_aggregated_state(
    target: Dict[Tuple[str, Tuple[str, ...]], AggregatedCandidate],
    items: Iterable[AggregatedCandidate],
) -> None:
    for item in items:
        key = (item.normalized, item.parents)
        if key not in target:
            target[key] = AggregatedCandidate(
                level=item.level,
                normalized=item.normalized,
                parents=item.parents,
                primary_label=item.primary_label,
                aliases=set(item.aliases),
                record_fingerprints=set(item.record_fingerprints),
                institutions=set(item.institutions),
                total_count=item.total_count,
            )
            continue
        existing = target[key]
        existing.aliases.update(item.aliases)
        existing.record_fingerprints.update(item.record_fingerprints)
        existing.institutions.update(item.institutions)
        existing.total_count += item.total_count
        if not existing.primary_label.strip():
            existing.primary_label = item.primary_label


def _stream_candidates(candidates: Sequence[Candidate], output_path: str | Path) -> Path:
    path = Path(output_path)
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        for candidate in candidates:
            handle.write(json.dumps(candidate.model_dump(), sort_keys=True))
            handle.write("\n")
    return path.resolve()


def _load_resume_checkpoint(
    path: Path | None,
) -> Tuple[int, Dict[Tuple[str, Tuple[str, ...]], AggregatedCandidate]]:
    if path is None or not path.exists():
        return 0, {}
    payload = json.loads(path.read_text(encoding="utf-8"))
    processed = int(payload.get("processed_records", 0))
    aggregated: Dict[Tuple[str, Tuple[str, ...]], AggregatedCandidate] = {}
    for entry in payload.get("aggregated", []):
        parents = tuple(entry.get("parents", []))
        key = (entry["normalized"], parents)
        aggregated[key] = AggregatedCandidate(
            level=int(entry.get("level", 0)),
            normalized=entry["normalized"],
            parents=parents,
            primary_label=entry.get("primary_label", ""),
            aliases=set(entry.get("aliases", [])),
            record_fingerprints=set(entry.get("record_fingerprints", [])),
            institutions=set(entry.get("institutions", [])),
            total_count=int(entry.get("total_count", 0)),
        )
    return processed, aggregated


def _write_resume_checkpoint(
    path: Path,
    processed_records: int,
    aggregated: Dict[Tuple[str, Tuple[str, ...]], AggregatedCandidate],
) -> None:
    data = {
        "processed_records": processed_records,
        "aggregated": [
            {
                "level": item.level,
                "normalized": item.normalized,
                "parents": list(item.parents),
                "primary_label": item.primary_label,
                "aliases": sorted(item.aliases),
                "record_fingerprints": sorted(item.record_fingerprints),
                "institutions": sorted(item.institutions),
                "total_count": item.total_count,
            }
            for item in aggregated.values()
        ],
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, indent=2) + "\n", encoding="utf-8")


def _parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run S1 extraction and normalization")
    parser.add_argument("source_records", type=Path, help="Path to S0 JSONL records")
    parser.add_argument("level", type=int, choices=range(0, 4), help="Hierarchy level to process")
    parser.add_argument("output", type=Path, help="Destination JSONL path for candidates")
    parser.add_argument(
        "--parents",
        type=Path,
        help="Optional JSON file containing previously promoted parent candidates",
    )
    parser.add_argument(
        "--metadata",
        type=Path,
        help="Optional metadata output path",
    )
    parser.add_argument(
        "--resume-from",
        type=Path,
        help="Optional checkpoint file to resume from",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="Number of records to process per extraction batch",
    )
    parser.add_argument(
        "--audit-mode",
        action="store_true",
        help="Limit S1 extraction to 10 source records for audit verification",
    )
    return parser.parse_args(argv)


def _load_parents(path: Path | None) -> List[Candidate]:
    if path is None or not path.exists():
        return []
    payload = json.loads(path.read_text(encoding="utf-8"))
    return [Candidate.model_validate(item) for item in payload]


def main(argv: Sequence[str] | None = None) -> int:
    args = _parse_args(argv)
    logger = get_logger(module=__name__)

    try:
        parents = _load_parents(args.parents)
        extract_candidates(
            args.source_records,
            level=args.level,
            previous_parents=parents,
            output_path=args.output,
            metadata_path=args.metadata,
            resume_from=args.resume_from,
            batch_size=args.batch_size,
            audit_mode=args.audit_mode,
        )
    except Exception as exc:  # pragma: no cover - defensive CLI guard
        logger.exception("S1 extraction failed", error=str(exc))
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
