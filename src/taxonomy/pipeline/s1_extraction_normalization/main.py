"""Public entry points for running the S1 extraction pipeline."""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, Iterable, Iterator, List, Sequence, Tuple

from taxonomy.config.settings import Settings
from taxonomy.entities.core import Candidate, Concept
from taxonomy.observability import ObservabilityContext
from taxonomy.utils.logging import get_logger, logging_context

from .extractor import ExtractionProcessor
from .io import generate_metadata, load_source_records
from .normalizer import CandidateNormalizer
from .parent_index import ParentIndex
from .processor import AggregatedCandidate, S1Processor
from taxonomy.utils.helpers import chunked


def extract_candidates(
    source_records_path: str | Path,
    *,
    level: int,
    previous_parents: Sequence[Candidate | Concept] | None = None,
    output_path: str | Path | None = None,
    metadata_path: str | Path | None = None,
    resume_from: str | Path | None = None,
    batch_size: int = 32,
    settings: Settings | None = None,
    observability: ObservabilityContext | None = None,
) -> List[Candidate]:
    """High-level API for running the S1 pipeline.

    Args:
        source_records_path: Path to the JSONL file generated by S0.
        level: Hierarchy level to process (0..3).
        previous_parents: Optional parents emitted by earlier levels to assist
            parent resolution.
        output_path: Optional destination for candidate JSONL output.
        metadata_path: Optional metadata output file. When omitted the path is
            derived from *output_path* by appending ``.metadata.json``.
        settings: Optional pre-loaded :class:`Settings` instance.
        observability: Optional observability context used for standardized
            metrics, evidence, and quarantine tracking.
    """

    cfg = settings or Settings()
    label_policy = cfg.policies.label_policy
    extractor = ExtractionProcessor(observability=observability)
    normalizer = CandidateNormalizer(label_policy=label_policy)
    parent_index = ParentIndex(
        label_policy=label_policy,
        similarity_cutoff=label_policy.parent_similarity_cutoff,
    )
    processor = S1Processor(
        extractor=extractor,
        normalizer=normalizer,
        parent_index=parent_index,
    )

    obs_context = observability or extractor.observability
    snapshot_before = obs_context.snapshot() if obs_context is not None else None
    before_counters = snapshot_before.counters.get("S1", {}) if snapshot_before else {}
    before_op_sequence = 0
    if snapshot_before:
        s1_ops_before = [
            op for op in snapshot_before.operations
            if isinstance(op, dict) and op.get("phase") == "S1"
        ]
        if s1_ops_before:
            before_op_sequence = int(s1_ops_before[-1].get("sequence", 0))
    before_quarantine_sequence = 0
    if snapshot_before:
        items_before = snapshot_before.quarantine.get("items", [])
        s1_items_before = [
            it for it in items_before
            if isinstance(it, dict) and it.get("phase") == "S1"
        ]
        if s1_items_before:
            before_quarantine_sequence = int(s1_items_before[-1].get("sequence", 0))
    previous_parents = list(previous_parents or [])
    if previous_parents:
        parent_index.build_index(previous_parents)

    total_records: int | None = None
    if cfg.observability.precount_s1_records:
        total_records = sum(1 for _ in load_source_records(source_records_path))

    resume_path = Path(resume_from) if resume_from is not None else None
    processed_records, aggregated_state = _load_resume_checkpoint(resume_path)
    if total_records is not None and processed_records > total_records:
        processed_records = total_records

    remaining_records = _skip_records(load_source_records(source_records_path), processed_records)

    with logging_context(stage="s1", level=level, records=total_records):
        for batch in chunked(remaining_records, batch_size):
            raw = extractor.extract_candidates(batch, level=level, observability=obs_context)
            normalized = normalizer.normalize(raw, level=level)
            aggregated_batch = processor._aggregate(normalized)
            _merge_aggregated_state(aggregated_state, aggregated_batch)
            processed_records += len(batch)
            if resume_path is not None:
                _write_resume_checkpoint(resume_path, processed_records, aggregated_state)

        provider_errors = sum(
            1
            for entry in snapshot_after.operations
            if isinstance(entry, dict)
            and entry.get("phase") == "S1"
            and entry.get("operation") == "provider_error"
            and int(entry.get("sequence", 0)) > before_op_sequence
        )
        quarantined = 0
        items_after = snapshot_after.quarantine.get("items", [])
        for item in items_after:
            if (
                isinstance(item, dict)
                and item.get("phase") == "S1"
                and int(item.get("sequence", 0)) > before_quarantine_sequence
            ):
                quarantined += 1
                "invalid_json": _delta("invalid_json"),
                "quarantined": quarantined,
                "provider_errors": int(provider_errors),
                "retries": _delta("retries"),
                "final_candidates": len(candidates),
            }
        else:
            metrics = extractor.metrics
            stats = {
                "records_in": metrics.records_in,
                "records_processed_total": processed_records,
                "candidates_out": metrics.candidates_out,
                "invalid_json": metrics.invalid_json,
                "quarantined": metrics.quarantined,
                "provider_errors": metrics.provider_errors,
                "retries": metrics.retries,
                "final_candidates": len(candidates),
            }
        config_used = {
            "policy_version": cfg.policies.policy_version,
            "level": level,
            "batch_size": batch_size,
        }
        metadata = generate_metadata(stats, config_used)
        Path(metadata_path).write_text(json.dumps(metadata, indent=2) + "\n", encoding="utf-8")

    if resume_path is not None:
        _write_resume_checkpoint(resume_path, processed_records, aggregated_state)

    return candidates


def _skip_records(records: Iterable, offset: int) -> Iterator:
    for index, record in enumerate(records):
        if index < offset:
            continue
        yield record


def _merge_aggregated_state(
    target: Dict[Tuple[str, Tuple[str, ...]], AggregatedCandidate],
    items: Iterable[AggregatedCandidate],
) -> None:
    for item in items:
        key = (item.normalized, item.parents)
        if key not in target:
            target[key] = AggregatedCandidate(
                level=item.level,
                normalized=item.normalized,
                parents=item.parents,
                primary_label=item.primary_label,
                aliases=set(item.aliases),
                record_fingerprints=set(item.record_fingerprints),
                institutions=set(item.institutions),
                total_count=item.total_count,
            )
            continue
        existing = target[key]
        existing.aliases.update(item.aliases)
        existing.record_fingerprints.update(item.record_fingerprints)
        existing.institutions.update(item.institutions)
        existing.total_count += item.total_count
        if not existing.primary_label.strip():
            existing.primary_label = item.primary_label


def _stream_candidates(candidates: Sequence[Candidate], output_path: str | Path) -> Path:
    path = Path(output_path)
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        for candidate in candidates:
            handle.write(candidate.model_dump_json(sort_keys=True))
            handle.write("\n")
    return path.resolve()


def _load_resume_checkpoint(
    path: Path | None,
) -> Tuple[int, Dict[Tuple[str, Tuple[str, ...]], AggregatedCandidate]]:
    if path is None or not path.exists():
        return 0, {}
    payload = json.loads(path.read_text(encoding="utf-8"))
    processed = int(payload.get("processed_records", 0))
    aggregated: Dict[Tuple[str, Tuple[str, ...]], AggregatedCandidate] = {}
    for entry in payload.get("aggregated", []):
        parents = tuple(entry.get("parents", []))
        key = (entry["normalized"], parents)
        aggregated[key] = AggregatedCandidate(
            level=int(entry.get("level", 0)),
            normalized=entry["normalized"],
            parents=parents,
            primary_label=entry.get("primary_label", ""),
            aliases=set(entry.get("aliases", [])),
            record_fingerprints=set(entry.get("record_fingerprints", [])),
            institutions=set(entry.get("institutions", [])),
            total_count=int(entry.get("total_count", 0)),
        )
    return processed, aggregated


def _write_resume_checkpoint(
    path: Path,
    processed_records: int,
    aggregated: Dict[Tuple[str, Tuple[str, ...]], AggregatedCandidate],
) -> None:
    data = {
        "processed_records": processed_records,
        "aggregated": [
            {
                "level": item.level,
                "normalized": item.normalized,
                "parents": list(item.parents),
                "primary_label": item.primary_label,
                "aliases": sorted(item.aliases),
                "record_fingerprints": sorted(item.record_fingerprints),
                "institutions": sorted(item.institutions),
                "total_count": item.total_count,
            }
            for item in aggregated.values()
        ],
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, indent=2) + "\n", encoding="utf-8")


def _parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run S1 extraction and normalization")
    parser.add_argument("source_records", type=Path, help="Path to S0 JSONL records")
    parser.add_argument("level", type=int, choices=range(0, 4), help="Hierarchy level to process")
    parser.add_argument("output", type=Path, help="Destination JSONL path for candidates")
    parser.add_argument(
        "--parents",
        type=Path,
        help="Optional JSON file containing previously promoted parent candidates",
    )
    parser.add_argument(
        "--metadata",
        type=Path,
        help="Optional metadata output path",
    )
    parser.add_argument(
        "--resume-from",
        type=Path,
        help="Optional checkpoint file to resume from",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="Number of records to process per extraction batch",
    )
    return parser.parse_args(argv)


def _load_parents(path: Path | None) -> List[Candidate]:
    if path is None or not path.exists():
        return []
    payload = json.loads(path.read_text(encoding="utf-8"))
    return [Candidate.model_validate(item) for item in payload]


def main(argv: Sequence[str] | None = None) -> int:
    args = _parse_args(argv)
    logger = get_logger(module=__name__)

    try:
        parents = _load_parents(args.parents)
        extract_candidates(
            args.source_records,
            level=args.level,
            previous_parents=parents,
            output_path=args.output,
            metadata_path=args.metadata,
            resume_from=args.resume_from,
            batch_size=args.batch_size,
        )
    except Exception as exc:  # pragma: no cover - defensive CLI guard
        logger.exception("S1 extraction failed", error=str(exc))
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
